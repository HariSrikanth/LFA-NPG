# Abstract:
Neural Network based approximations of the Value function make up the core of leading Policy Based methods such as Trust Regional Policy Optimization (TRPO) and Proximal Policy Optimization (PPO).  While this adds significant value when dealing with very complex environments, we note that in sufficiently low state and action space environments, a computationally expensive Neural Network architecture offers marginal improvement over simpler Value approximation methods. We present an implementation of Natural Actor Critic algorithms with actor updates through Natural Policy Gradient methods. This paper proposes that Natural Policy Gradient (NPG) methods with Linear Function Approximation as a paradigm for value approximation may surpass the performance and speed of Neural Network based models such as TRPO and PPO within these environments. Over Reinforcement Learning benchmarks Cart Pole and Acrobot, we observe that our algorithm trains much faster than complex neural network architectures, and obtains an equivalent or greater result. This allows us to recommend the use of NPG methods with Linear Function Approximation over TRPO and PPO for both traditional and sparse reward low dimensional problems. 

Refer to https://github.com/HariSrikanth/LFA-NPG/blob/baf0f89ddb678094f9307716f20f69c0944320e2/Linear_Function_Approximation_as_a_Computationally_Efficient_Method_to_solve_Classical_Reinforcement_Learning_Challenges%20(3).pdf for more detail.
